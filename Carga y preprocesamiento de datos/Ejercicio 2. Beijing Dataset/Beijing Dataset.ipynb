{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Beijing Multi-Site [Air Quality Data](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este conjunto de datos no tendremos que hacer un esfuerzo muy grande en lo relativo a estudiar la *metadata*, pero exploraremos una serie de comandos de Linux que nos será muy útil conocer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La manera de Andrés (no me funciona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movemos el directorio activo a una nueva localización para este dataset\n",
    "## Retrocedemos un nivel\n",
    "%cd ..\n",
    "## Creamos carpeta\n",
    "!mkdir /content/air_quality_dataset\n",
    "## Movemos directorio activo\n",
    "%cd /content/air_quality_dataset\n",
    "# Descargamos fichero comprimido\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip\n",
    "# Descargamos el fichero que contiene los datos a nuestro directorio activo\n",
    "!unzip PRSA2017_Data_20130301-20170228.zip\n",
    "# Nos movemos a la carpeta que contenía el zip\n",
    "%cd PRSA_Data_20130301-20170228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La manera de Demetrio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducimos los datos: https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo primero que vemos aqui, es que la url ya no conduce a los datos, con lo cual dentro de esa pagina, en su buscador escribimos beijing y accedemos a los datos\n",
    "\n",
    "nueva_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip'\n",
    "\n",
    "#tenemos una extension .zip, quiere decir que es un archivo comprimido, por lo que necesitamos descomprimirlo\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "#vamos a crear una carpeta donde guardaremos los datos\n",
    "import os\n",
    "\n",
    "#creamos la carpeta\n",
    "os.makedirs('./beijing', exist_ok=True)\n",
    "\n",
    "#descargamos el archivo\n",
    "r = requests.get(nueva_url)\n",
    "\n",
    "#creamos un objeto zipfile\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "#extraemos los archivos\n",
    "z.extractall('./beijing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el dataframe con los datos. Como se puede ver en los csv, todos los datos tienen las mismas columnas asi que se pueden concatenar sin problema para formar una única masa de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#creamos un dataframe vacio\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#iteramos sobre los archivos de la carpeta beijing y los leemos con pandas(recuerden que al descomprimir, se creo \n",
    "#una carpeta nueva C:\\Users\\demst\\OneDrive\\Escritorio\\preprocesamiento\\beijing\\PRSA_Data_20130301-20170228)\n",
    "\n",
    "for file in os.listdir('./beijing/PRSA_Data_20130301-20170228'):            #Por cada archivo en nuestra carpeta del notebook:\n",
    "    if file.endswith('.csv'):                                               #Si el archivo termina en .csv:\n",
    "        df = pd.concat([df, pd.read_csv('./beijing/PRSA_Data_20130301-20170228/' + file)])  #leemos el archivo y lo concatenamos a nuestro dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # echamos un vistazo a los primeros y ultimos datos, así como a sus dimensiones (420768 filas por 18 columnas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Observamos los datos nulos en cada variable/columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ya tenemos nuestro df, ahora vamos a comprobar los valores repetidos y los valores nulos\n",
    "print(df.duplicated().sum()) # df.duplicated indica un booleano si hay valores duplicados, y sum() los agrega. Como se puede ver, no hay valores repes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sabemos que no hay valores repetidos, pero, ¿qué hay de los valores nulos? En la tabla anterior (df.info()) hemos visto que sí hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nulos por agregación\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nulos por % \n",
    "print(df.isnull().sum()/len(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, parece que variables como NO2, CO u O3, entre otros, tienen gran cantidad de datos faltantes, pero en términos relativos no suponen ni el 5% del total. Esto también es importante tenerlo en cuenta para saber si seremos capaces o si sería responsable tomar la decisión de sustituir los datos faltantes a raíz de los existentes (podemos rellenar el 2% con el otro 98%, pero no podemos rellenar un 85% con un 15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos de que tipo son nuestros datos\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reparamos los valores nulos usando la super-mega-funcion de andres para reparar float, objets y ints, ya que representan \n",
    "#muy poco % de los datos y no afectaran el modelo. Si el % fuera mayor (por ejemplo, un 20%) podriamos usar el algoritmo knn\n",
    "\n",
    "def impute_missing_values(df):\n",
    "    for col in df.columns:  # Para cada columna en df:\n",
    "        if df[col].dtype == 'float64':  # Si el tipo de dato es float64:\n",
    "            df[col] = df[col].fillna(df[col].mean()) # Sustituimos los valores nulos con la MEDIA de la columna\n",
    "        elif df[col].dtype == 'object': # Si el tipo de dato es object (señal de que suele ser string):\n",
    "            df[col] = df[col].fillna(df[col].mode()[0]) # Sustituimos los valores nulos con la MODA de la columna\n",
    "        else: # Si no es ninguno de los anteriores (en este caso, viendo la tabla anterior, si es un int):\n",
    "            df[col] = df[col].fillna(df[col].median()) # Sustituimos los valores nulos con la MEDIANA de la columna\n",
    "    return df\n",
    "\n",
    "df = impute_missing_values(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos que ya no hay valores nulos\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así hemos arreglado la base de datos. Además, al haber tantos datos (muestra de 420k datos en total) y pocos datos nulos (4,92% en el peor de los casos) no es especialmente descabellado proponer como método de sustitución de valores nulos la media, moda y mediana según corresponda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis estadístico de los datos (Explayarse todo lo que uno quiera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora te toca, ¿eres capaz de leer todos los `csv`, concatenarlos y construir un `pd.DataFrame` en una sola línea de código?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df = pd.concat([pd.read_csv(elem) for elem in os.listdir()]).reset_index(drop=True)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
